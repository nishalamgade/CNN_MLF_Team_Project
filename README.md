# CNN Implementation for MNIST Digit Recognition
 
## Team Members

- Prayukti Dahal - 101145484
- Anushuya Baidya - 101163891
- Binamra Neupane - 101169010
- Nisha Lamgade - 101161837
- Manisha Senchuri - 101162804

## Introduction

In computer vision (CV) and pattern recognition (CR), accurately recognizing handwritten digits is one of the most important tasks to learn. For this group project, we will provide a deep-dive implementation of a CNN model specifically designed for the optical detection of handwritten numbers. The model will be trained and validated on the well-known MNIST dataset that has become the standard for image classification challenges.

## Dataset - MNIST

MNIST (Modified National Institute of Standards and Technology database) dataset is a large grayscale dataset that contains 70,000 images of handwritten digits from 0 through 9. Each image in the dataset has a 28x28 pixel resolution. The dataset is split into two subsets: training set (60,000 images) and test set (10,000 images).

## Data Preprocessing

Before feeding the data into the CNN model, several preprocessing steps are performed:

1. **Data Fetching:** The MNIST dataset is fetched from the UCI Machine Learning Repository using the `fetch_ucirepo` function from the `ucimlrepo` library.
2. **Data Splitting:** The dataset is split into features (input images) and targets (digit labels), denoted as X and y respectively.
3. **Reshaping:** The input images (X) are reshaped into a 4D tensor format suitable for the CNN model. The original 2D images are transformed into a 3D tensor with an additional channel dimension (grayscale images have a single channel).
4. **Normalization:** The pixel values of the input images are normalized to a range of [0, 1] by dividing each pixel value by the maximum possible value (16.0 in this case).
5. **Train-Test Split:** The pre-processed dataset is further divided into training and testing sets. In this implementation, 80% of the data is allocated for training, and the remaining 20% is used for testing.

## CNN Model Architecture

The architecture of the CNN model, constructed using TensorFlow's Keras API, serves as a pivotal tool for recognizing handwritten digits with precision. Here's an overview of the layer stack:

1. **Convolutional Layer 1:** This initial layer employs 32 convolutional filters (3x3) to process input images, activating ReLU and using same padding. It extracts fundamental features from the data.
2. **Max Pooling Layer 1:** Utilizing a 2x2 maximum pooling operation, this layer down samples feature maps retaining essential details.
3. **Convolutional Layer 2:** Following up, this layer adds 64 convolutional filters (3x3), also employing ReLU activation and same padding. It refines features to a higher level.
4. **Max Pooling Layer 2:** Another round of 2x2 max pooling further compresses feature maps, preserving crucial information.
5. **Flatten Layer:** This step converts multidimensional feature maps into a 1D vector, preparing them for subsequent processing.
6. **Dense Layer 1:** Featuring 64 fully connected units with ReLU activation, this layer integrates features from earlier convolutional layers.
7. **Dense Layer 2 (Output Layer):** Finally, the output layer comprises 10 units, one for each digit class, utilizing softmax activation to generate class probabilities.

## Model Training

The CNN model is compiled with the following configurations:

1. **Optimizer:** The Adam optimizer is an add-on to the Stochastic gradient descent algorithm that is used to train the model's weights.
2. **Loss Function:** The loss function is sparse and categorical, which is appropriate for multidimensional classification problems with integers encoded labels.
3. **Metrics:** The accuracy, precision, recall and f1-score metrics evaluate how well the model performs in training and testing.

Using the training data, the model is trained for a specific number of iterations (for example, 10 iterations in this implementation). During the training phase, the test set is used to track the modelâ€™s performance and avoid overfitting.

## Model Evaluation

After training, the model's accuracy is evaluated on the test set, providing a measure of its performance on unseen data. Additionally, the training and validation loss curves are plotted to visualize the model's training progress and convergence.

## Effect of Max Pooling on Convolutional Layer Feature Maps in the CNN model

We accessed the outputs of the first and second convolutional layers of the convolutional neural network (CNN) model and demonstrated the effect of max pooling on each layer's feature maps.

- For the first convolutional layer, we retrieved its output using `conv1_output`. This output represents the feature maps generated by applying convolutional filters to the input images. We then visualized both the original feature maps and the corresponding max-pooled feature maps. The max-pooled images show a reduced spatial dimensionality compared to the original feature maps, achieved through down sampling using a max pooling operation. This down sampling helps in preserving the most significant features while reducing computational complexity and improving translational invariance.

- Similarly, we repeat the process for the second convolutional layer, accessing its output using `conv2_output`. Again, we visualized both the original and max-pooled feature maps. Through this process, we observed how max pooling further compresses the feature maps generated by the second convolutional layer, effectively capturing the most relevant features in a more compacted representation.

Overall, the application of max pooling after each convolutional layer in the CNN architecture helps in feature extraction by retaining essential information while reducing spatial dimensions, ultimately contributing to improved model performance in tasks such as image classification and object (digit) recognition in case of our project.

## Training and Validation Loss Curve

We plotted the training and validation loss curves to visually track how our model is improving during training over different epochs.

## Evaluating Model Performance Metrics on Test Data

We evaluated the performance of the trained model on the test dataset by predicting labels for the test images and calculating key evaluation metrics. These metrics include Accuracy, Precision, Recall, and F1-score, providing insights into the model's classification performance across different aspects such as correctness, completeness, and overall effectiveness in handling various classes within the dataset.

## Visualizing Model Performance with Confusion Matrix

Following the evaluation of model performance metrics, we generated a confusion matrix to provide a detailed understanding of the model's classification results. The confusion matrix illustrates the true labels against the predicted labels, allowing for a comprehensive analysis of the model's performance across different classes. The heatmap visualization with remarks facilitates easy interpretation, with darker shades indicating higher frequencies of correct or incorrect classifications. This visual representation advances in identifying any patterns of misclassifications and areas where the model may require improvement.

## K-fold Cross-Validation

The K-fold cross-validation was done to evaluate the model's performance, splitting the training data into five subsets, and iteratively training and evaluating the model on each fold. Finally, it calculated and displayed the average accuracy across all folds.
